{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>On days when I feel close to my partner and ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>Every time I imagine that someone I love or I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I had been obviously unjustly treated and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I think about the short time that we live...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>At a gathering I found myself involuntarily si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Target                                           Sentence\n",
       "0      joy  On days when I feel close to my partner and ot...\n",
       "1     fear  Every time I imagine that someone I love or I ...\n",
       "2    anger  When I had been obviously unjustly treated and...\n",
       "3  sadness  When I think about the short time that we live...\n",
       "4  disgust  At a gathering I found myself involuntarily si..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_csv = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\n",
    "df = pd.read_csv(\"1-P-3-ISEAR.csv\",header=None)\n",
    "\n",
    "\n",
    "df.columns = ['sn','Target','Sentence']\n",
    "df.drop('sn',inplace=True,axis =1)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (5584,)\n",
      "shape of test data is (1862,)\n"
     ]
    }
   ],
   "source": [
    "X,y = df['Sentence'].values,df['Target'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "print(f'shape of train data is {x_train.shape}')\n",
    "print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAXF0lEQVR4nO3df5TcdX3v8eerrBBBTAhZONwkGCp7VXp6QNhiFKuUUI5gNWklR6hKoLnd2oKWYlu5V6/aH9YfeEsvl1vaVLgEVDCgmEg5aG4gYNUgmxASINqs/Ei2ocmiJEojVeTdP77vLZPN7O53d2d24cPrcc6c+Xw/38985z3fmXntdz4zs6OIwMzMyvILU12AmZm1nsPdzKxADnczswI53M3MCuRwNzMrUMdUFwAwa9asmDdv3lSXYWb2grJ+/fonIqKz2brnRbjPmzeP3t7eqS7DzOwFRdJjw63ztIyZWYEc7mZmBXK4m5kVyOFuZlYgh7uZWYEc7mZmBXK4m5kVyOFuZlagWuEu6Y8kPSjpAUk3SJom6RhJ90jaKumLkg7MsQflcl+un9fOG2BmZvsb9RuqkmYD7weOi4ifSFoBnAOcBVweETdK+jtgKXBVnj8ZEcdKOgf4FPDOsRZ20p9cN9aLTIr1l5031SWYmY2q7r8f6ABeKulnwMHA48BpwG/n+uXAx6jCfWG2AW4GrpSkeJH95NO2P//lqS6hqaM/snmqSzCzSTBquEfEv0j6DLAN+AnwdWA9sDsinslh/cDsbM8Gtudln5G0BzgceKLFtVsbnfJ/TpnqEpr65vu+OdUlmL0g1JmWOYzqaPwYYDdwE3Bmk6GDR+YaYV3jdnuAHoCjjz66Zrlmo7vrTW+e6hKG9ea775rqEuxFos60zOnAIxExACDpy8AbgBmSOvLofQ6wI8f3A3OBfkkdwHTgh0M3GhHLgGUA3d3dL6opG7ORXPmBr051CU1d9L/eVmvcx999dpsrGZ8Pfe7mqS5hUtX5tMw2YL6kgyUJWAA8BNwJDN6LS4CV2V6Vy+T6O15s8+1mZlNt1HCPiHuo3hjdAGzOyywDPghcIqmPak796rzI1cDh2X8JcGkb6jYzsxHU+rRMRHwU+OiQ7oeBk5uMfRpYPPHSzMxsvJ4Xv8RkZvZ8seXjd0x1CU295kOnjWm8//2AmVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWoFHDXdKrJG1sOP1I0sWSZkpaLWlrnh+W4yXpCkl9kjZJOrH9N8PMzBrV+Q3V70XECRFxAnASsBe4heq3UddERBewhud+K/VMoCtPPcBV7SjczMyGN9ZpmQXA9yPiMWAhsDz7lwOLsr0QuC4q64AZko5qSbVmZlbLWMP9HOCGbB8ZEY8D5PkR2T8b2N5wmf7s24ekHkm9knoHBgbGWIaZmY2kdrhLOhB4O3DTaEOb9MV+HRHLIqI7Iro7OzvrlmFmZjWM5cj9TGBDROzM5Z2D0y15viv7+4G5DZebA+yYaKFmZlbfWML9XJ6bkgFYBSzJ9hJgZUP/efmpmfnAnsHpGzMzmxwddQZJOhj4deD3Gro/CayQtBTYBizO/tuAs4A+qk/WXNCyas3MrJZa4R4Re4HDh/T9gOrTM0PHBnBhS6ozM7Nx8TdUzcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAtUKd0kzJN0s6buStkh6vaSZklZL2prnh+VYSbpCUp+kTZJObO9NMDOzoeoeuf9v4PaIeDVwPLAFuBRYExFdwJpchuqHtLvy1ANc1dKKzcxsVKOGu6SXA28CrgaIiJ9GxG5gIbA8hy0HFmV7IXBdVNYBMyQd1fLKzcxsWHWO3H8RGAD+n6T7JH1W0iHAkRHxOECeH5HjZwPbGy7fn31mZjZJ6oR7B3AicFVEvBb4N56bgmlGTfpiv0FSj6ReSb0DAwO1ijUzs3rqhHs/0B8R9+TyzVRhv3NwuiXPdzWMn9tw+TnAjqEbjYhlEdEdEd2dnZ3jrd/MzJoYNdwj4l+B7ZJelV0LgIeAVcCS7FsCrMz2KuC8/NTMfGDP4PSNmZlNjo6a494HfF7SgcDDwAVUfxhWSFoKbAMW59jbgLOAPmBvjjUzs0lUK9wjYiPQ3WTVgiZjA7hwgnWZmdkE+BuqZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFqhXukh6VtFnSRkm92TdT0mpJW/P8sOyXpCsk9UnaJOnEdt4AMzPb31iO3H8tIk6IiMGf27sUWBMRXcCaXAY4E+jKUw9wVauKNTOzeiYyLbMQWJ7t5cCihv7rorIOmCHpqAlcj5mZjVHdcA/g65LWS+rJviMj4nGAPD8i+2cD2xsu2599+5DUI6lXUu/AwMD4qjczs6Y6ao47JSJ2SDoCWC3puyOMVZO+2K8jYhmwDKC7u3u/9WZmNn61jtwjYkee7wJuAU4Gdg5Ot+T5rhzeD8xtuPgcYEerCjYzs9GNGu6SDpF06GAbOAN4AFgFLMlhS4CV2V4FnJefmpkP7BmcvjEzs8lRZ1rmSOAWSYPjvxARt0u6F1ghaSmwDVic428DzgL6gL3ABS2v2szMRjRquEfEw8DxTfp/ACxo0h/AhS2pzszMxsXfUDUzK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK1DtcJd0gKT7JN2ay8dIukfSVklflHRg9h+Uy325fl57Sjczs+GM5cj9D4EtDcufAi6PiC7gSWBp9i8FnoyIY4HLc5yZmU2iWuEuaQ7wVuCzuSzgNODmHLIcWJTthblMrl+Q483MbJLUPXL/G+BPgWdz+XBgd0Q8k8v9wOxszwa2A+T6PTl+H5J6JPVK6h0YGBhn+WZm1syo4S7pN4BdEbG+sbvJ0Kix7rmOiGUR0R0R3Z2dnbWKNTOzejpqjDkFeLuks4BpwMupjuRnSOrIo/M5wI4c3w/MBfoldQDTgR+2vHIzMxvWqEfuEfHfI2JORMwDzgHuiIh3AXcCZ+ewJcDKbK/KZXL9HRGx35G7mZm1z0Q+5/5B4BJJfVRz6ldn/9XA4dl/CXDpxEo0M7OxqjMt858iYi2wNtsPAyc3GfM0sLgFtZmZ2Tj5G6pmZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmB6vxA9jRJ35F0v6QHJf1Z9h8j6R5JWyV9UdKB2X9QLvfl+nntvQlmZjZUnSP3fwdOi4jjgROAt0iaD3wKuDwiuoAngaU5finwZEQcC1ye48zMbBLV+YHsiIincvEleQrgNODm7F8OLMr2wlwm1y+QpJZVbGZmo6o15y7pAEkbgV3AauD7wO6IeCaH9AOzsz0b2A6Q6/dQ/YD20G32SOqV1DswMDCxW2FmZvuoFe4R8fOIOAGYQ/Wj2K9pNizPmx2lx34dEcsiojsiujs7O+vWa2ZmNYzp0zIRsRtYC8wHZkjqyFVzgB3Z7gfmAuT66cAPW1GsmZnVU+fTMp2SZmT7pcDpwBbgTuDsHLYEWJntVblMrr8jIvY7cjczs/bpGH0IRwHLJR1A9cdgRUTcKukh4EZJfwncB1yd468GrpfUR3XEfk4b6jYzsxGMGu4RsQl4bZP+h6nm34f2Pw0sbkl1ZmY2Lv6GqplZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZger8hupcSXdK2iLpQUl/mP0zJa2WtDXPD8t+SbpCUp+kTZJObPeNMDOzfdU5cn8G+EBEvAaYD1wo6TjgUmBNRHQBa3IZ4EygK089wFUtr9rMzEY0arhHxOMRsSHbPwa2ALOBhcDyHLYcWJTthcB1UVkHzJB0VMsrNzOzYY1pzl3SPKofy74HODIiHofqDwBwRA6bDWxvuFh/9g3dVo+kXkm9AwMDY6/czMyGVTvcJb0M+BJwcUT8aKShTfpiv46IZRHRHRHdnZ2ddcswM7MaaoW7pJdQBfvnI+LL2b1zcLolz3dlfz8wt+Hic4AdrSnXzMzqqPNpGQFXA1si4q8bVq0ClmR7CbCyof+8/NTMfGDP4PSNmZlNjo4aY04B3gNslrQx+/4H8ElghaSlwDZgca67DTgL6AP2Ahe0tGIzMxvVqOEeEf9E83l0gAVNxgdw4QTrMjOzCfA3VM3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswLV+Zm9ayTtkvRAQ99MSaslbc3zw7Jfkq6Q1Cdpk6QT21m8mZk1V+fI/VrgLUP6LgXWREQXsCaXAc4EuvLUA1zVmjLNzGwsRg33iLgb+OGQ7oXA8mwvBxY19F8XlXXADElHtapYMzOrZ7xz7kdGxOMAeX5E9s8GtjeM68++/UjqkdQrqXdgYGCcZZiZWTOtfkO12Q9pR7OBEbEsIrojoruzs7PFZZiZvbiNN9x3Dk635Pmu7O8H5jaMmwPsGH95ZmY2HuMN91XAkmwvAVY29J+Xn5qZD+wZnL4xM7PJ0zHaAEk3AKcCsyT1Ax8FPgmskLQU2AYszuG3AWcBfcBe4II21GxmZqMYNdwj4txhVi1oMjaACydalJmZTYy/oWpmViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mVqC2hLukt0j6nqQ+SZe24zrMzGx4LQ93SQcA/xc4EzgOOFfSca2+HjMzG147jtxPBvoi4uGI+ClwI7CwDddjZmbDUPWb1i3coHQ28JaI+G+5/B7gdRFx0ZBxPUBPLr4K+F5LC9nXLOCJNm6/3Vz/1Hkh1w6uf6q1u/5XRERnsxUdbbgyNenb7y9IRCwDlrXh+vcjqTciuifjutrB9U+dF3Lt4Pqn2lTW345pmX5gbsPyHGBHG67HzMyG0Y5wvxfoknSMpAOBc4BVbbgeMzMbRsunZSLiGUkXAV8DDgCuiYgHW309YzQp0z9t5Pqnzgu5dnD9U23K6m/5G6pmZjb1/A1VM7MCOdzNzApUVLhL+tZU1zARkuZJemCq63gxkfR+SVskfX6qaxmNpI9J+mNJfy7p9Em4vkXt+Ha5pEclzWr1dp+vJHVLuiLb50u6Mttt2b+D2vE59ykTEW+Y6hqsNSSJ6j2hZ9t8VX8AnBkRj4x3A5IOiIift7CmEUXERybpqhYBtwIPTdL1FSkieoHeJqvaun9LO3J/SpXLJD0gabOkd+a66yUtbBj7eUlvb1Mdh0j6R0n3Zx3vlPQRSffm8rIMLySdlOO+DVzYsI3zJX1Z0u2Stkr6dMO6MyR9W9IGSTdJeln2f1LSQ5I2SfpM9i3O67xf0t0tuG1fkbRe0oP5LePB/f7xvI51ko7M/lfm8r15tPlUw3b+JPs3Sfqz7JuXR9F/C2xg3+9LtJykvwN+EVgl6UOSrsma7ht8rGRN38h9vUHSG7L/VEl3SvoCsLmNNX4o/wnf/6f6JjeSrs1vgg93nzfd71nzrQ3bvlLS+c22k7fz7cBlkjZKeuU469/vuZCr3pf7c7OkV+fYkyV9K/f/tyQN3t7z83H3VUmPSLpI0iU5bp2kmQ23+/Z8fH5jcLvtIOl/SvqupNWSblD1imqtpO5cP0vSo9neZ79nX0v274giopgT8BTwDmA11ccwjwS2AUcBbwa+kuOmA48AHW2q4x3APzQsTwdmNixfD7wt25uAN2f7MuCBbJ8PPJyXnQY8RhV2s4C7gUNy3AeBjwAzqf6Fw+AnoGbk+WZgdmPfBG/bzDx/KfAAcDjVN5AHb8+ngQ9n+1bg3Gy/F3gq22dQfURMVAcYtwJvAuYBzwLzJ/Ex82ju078C3j24n4B/Bg4BDgamZX8X0JvtU4F/A45pY20n5f13MPByoA/4Y+Ba4OwR7vPh9vupwK0N278yH2fDbeda4Ow2PBceBd6Xy38AfDbbLyefk8DpwJcangt9wKFAJ7AHeG+uuxy4ONtrgK5svw64o033SzewMZ8DhwJb835ZC3TnmFnAo0P3e96WK1u1f0c6FXXknt4I3BARP4+IncBdwK9ExF3AsZKOAM6leuA806YaNgOnS/qUpF+NiD3Ar0m6R9Jm4DTglyRNp3oi3ZWXu37IdtZExJ6IeJrqpdsrgPlU/23zm5I2Akuy/0fA08BnJf0WsDe38U3gWkm/S/UHb6LeL+l+YB3VH5su4KdUgQKwniqkAV4P3JTtLzRs44w83Ud1hP7q3A7AYxGxrgV1jtUZwKW5T9dS/UE9GngJ8A95v91Ete8HfScmMJ1Tw68Ct0TE3oj4Eft/GXC4+3y4/T6c4bbTCs2eCwBfzvPGx8t04CZV7ztdDvxSw3bujIgfR8QAVbh/tWH78/LV6xvy8huBv6c6qGuHNwIrI+InEfHjhlqeV4qac0/N/rfNoOuBd1F9a/Z32lVARPyzpJOAs4BPSPo61ZRLd0Rsl/QxqvAQTf7vToN/b2j/nOr+ErA6Is4dOljSycACqtt3EXBaRLxX0uuAtwIbJZ0QET8Yz+2SdCrVEdXrI2KvpLV5O34WeSjSUOeImwI+ERF/P2T786iOhqeCgHdExD7/wC7vq53A8VSvMp5uWD0ZtQ77+IjqC4P73ecjbOsZ9p2KnTbO7dQ2zHMBnntsNz5e/oIqxH8zHwtrGzbV+Fx4tmH52bz8LwC7I+KEVtQ9iuEypnH/TpuEOkZU4pH73cA7JR0gqZPq5f53ct21wMUA0cZvzUr6L8DeiPgc8BngxFz1RB5hnJ017Ab2SHpjrn9Xjc2vA06RdGxe18GS/mtud3pE3EZ1G0/I9a+MiHuiehPuCSY2jz0deDKD/dVUryJGq/Ud2T6nof9rwO/oufcKZucrqqn0Nap54MH3Ql6b/dOBx6N6Y/c9tObVT113A78p6aWSDgXe1rhyuPuc4ff7Y8Bxkg7KV40LRtnOj6mmHcZthOdCM9OBf8n2+WO5nnxl84ikxXm9knT82Cuu5Z+At0malvvurdn/KNVUGuRzfBQT3r8jKe3IPYBbqF6W3p/LfxoR/woQETslbQG+0uY6fpnqjZJngZ8Bv0/1zvhmqgfAvQ1jLwCukbSXKmBGFBED+SbYDZIOyu4PUz1QVkoafEXwR7nuMkld2beGar+M1+3AeyVtopqjHW365GLgc5I+APwj1ctpIuLrkl4DfDuz9Cng3VRHcVPlL4C/ATZlwD8K/Abwt8CXMjTuZBJfWUTEBklfpJrffQz4xpAhh9L8Ph9uv2+XtILqfZ6tVNNiI23nRqopqfdTzQ1/fxw3o9lz4eZhxn4aWC7pEuCOcVzXu4CrJH2YajrtRib2eG8qIu6VtCq3/RjVJ2H2UP3xWqHq35zXqb8V+3dYxfz7AUmHAxsi4hUjjDmYKmBPbJj7szbJ/f2TiAhJ51C9yecfbmkz7/f2k/SyiHgq9/XdQE9EbJjquhoVceSeL/3WUv3lHG7M6cA1wF872CfNScCVeSS8mza+z2H78H5vv2WqvoA0DVj+fAt2KOjI3czMnlPiG6pmZi96DnczswI53M3MCuRwNzMrkMPdzKxA/wHH7CfcqeGNcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dd = pd.Series(y_train).value_counts()\n",
    "\n",
    "# dd\n",
    "sns.barplot(x=np.array(['joy','sadness','anger','fear','disgust','shame','guilt']),y=dd.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tockenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tockenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
    "            \n",
    "            \n",
    "    print(type(y_train))\n",
    "            \n",
    "#     encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
    "#     encoded_test = [1 if label =='positive' else 0 for label in y_val] \n",
    "    \n",
    "    \n",
    "    \n",
    "#     #NAME_CONTRACT_TYPE ->catagorical to numerical\n",
    "\n",
    "    emotion = {        \n",
    "        'joy':1,\n",
    "        'sadness':2,\n",
    "        'anger':3,\n",
    "        'fear': 4,\n",
    "        'shame':5,\n",
    "        'disgust':6,\n",
    "        'guilt':0\n",
    "    }\n",
    "    encoded_train = [emotion[label] for label in y_train]\n",
    "\n",
    "\n",
    "    encoded_test = [emotion[label] for label in y_val]\n",
    "\n",
    "\n",
    "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 2, ..., 0, 0, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 0, ..., 3, 4, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary is 1000\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of vocabulary is {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    5584.000000\n",
       "mean        7.213467\n",
       "std         4.839897\n",
       "min         0.000000\n",
       "25%         4.000000\n",
       "50%         6.000000\n",
       "75%        10.000000\n",
       "max        53.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_len = [len(i) for i in x_train]\n",
    "pd.Series(rev_len).hist()\n",
    "plt.show()\n",
    "pd.Series(rev_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_pad = padding_(x_train, 9)\n",
    "x_test_pad = padding_(x_test,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 150\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 96, 258,   1,  25, 120, 212, 193, 657, 682], dtype=torch.int32),\n",
       " tensor(0, dtype=torch.int32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5584,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    811\n",
       "2    806\n",
       "3    802\n",
       "4    797\n",
       "6    794\n",
       "5    794\n",
       "0    780\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([150, 9])\n",
      "Sample input: \n",
      " tensor([[  0,   0, 143,  ..., 838, 351,  86],\n",
      "        [  0,   0,   0,  ..., 278, 211, 193],\n",
      "        [  0,   0,   0,  ..., 466,  36, 973],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ..., 403,  63, 227],\n",
      "        [  0,   0,   0,  ...,  73,  23, 130],\n",
      "        [  0,   0,   0,  ...,   0,   0,   6]], dtype=torch.int32)\n",
      "Sample input: \n",
      " tensor([4, 6, 1, 4, 5, 6, 2, 4, 4, 0, 3, 0, 3, 0, 4, 1, 3, 6, 2, 3, 6, 0, 3, 2,\n",
      "        0, 5, 5, 1, 5, 4, 1, 2, 4, 0, 4, 6, 4, 2, 3, 4, 3, 1, 3, 4, 5, 1, 5, 2,\n",
      "        4, 4, 6, 6, 1, 6, 2, 6, 2, 2, 3, 2, 5, 3, 2, 4, 2, 0, 3, 4, 1, 1, 4, 5,\n",
      "        0, 5, 4, 0, 4, 1, 1, 6, 1, 1, 3, 6, 6, 3, 3, 3, 0, 1, 4, 5, 0, 4, 3, 4,\n",
      "        3, 6, 4, 5, 5, 3, 4, 4, 4, 2, 5, 2, 4, 2, 0, 5, 1, 4, 1, 1, 2, 3, 6, 2,\n",
      "        4, 5, 3, 0, 6, 2, 4, 5, 2, 6, 1, 3, 2, 6, 6, 6, 5, 4, 2, 3, 1, 5, 2, 5,\n",
      "        0, 2, 5, 3, 5, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size())\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentLSTM,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(1001, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 7\n",
    "hidden_dim = 256\n",
    "\n",
    "model = SentimentLSTM(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150]) 2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-02275c29eee4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m#         print(labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m-> 1121\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2822\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2824\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "clip = 5\n",
    "epochs = 5 \n",
    "valid_loss_min = np.Inf\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    # initialize hidden state \n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "#         print(epoch)\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        \n",
    "#         print(inputs)\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "#         print(h)\n",
    "#         print(h[0])\n",
    "        model.zero_grad()\n",
    "        output,h = model(inputs,h)\n",
    "        \n",
    "        print(output.size() , len(h))\n",
    "        \n",
    "#         print(labels)\n",
    "    \n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "#         print(loss)\n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    " \n",
    "    \n",
    "    print(\"pint 2\")  \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            \n",
    "            print(outputsqueeze() , labels.float())\n",
    "            val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            accuracy = acc(output,labels)\n",
    "            val_acc += accuracy\n",
    "            \n",
    "#             print(labels)\n",
    "            \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '../working/state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
