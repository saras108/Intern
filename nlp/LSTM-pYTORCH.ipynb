{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>On days when I feel close to my partner and ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>Every time I imagine that someone I love or I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I had been obviously unjustly treated and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I think about the short time that we live...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>At a gathering I found myself involuntarily si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Target                                           Sentence\n",
       "0      joy  On days when I feel close to my partner and ot...\n",
       "1     fear  Every time I imagine that someone I love or I ...\n",
       "2    anger  When I had been obviously unjustly treated and...\n",
       "3  sadness  When I think about the short time that we live...\n",
       "4  disgust  At a gathering I found myself involuntarily si..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_csv = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\n",
    "df = pd.read_csv(\"1-P-3-ISEAR.csv\",header=None)\n",
    "\n",
    "\n",
    "df.columns = ['sn','Target','Sentence']\n",
    "df.drop('sn',inplace=True,axis =1)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (5584,)\n",
      "shape of test data is (1862,)\n"
     ]
    }
   ],
   "source": [
    "X,y = df['Sentence'].values,df['Target'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "print(f'shape of train data is {x_train.shape}')\n",
    "print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAXF0lEQVR4nO3df5TcdX3v8eerrBBBTAhZONwkGCp7VXp6QNhiFKuUUI5gNWklR6hKoLnd2oKWYlu5V6/aH9YfeEsvl1vaVLgEVDCgmEg5aG4gYNUgmxASINqs/Ei2ocmiJEojVeTdP77vLZPN7O53d2d24cPrcc6c+Xw/38985z3fmXntdz4zs6OIwMzMyvILU12AmZm1nsPdzKxADnczswI53M3MCuRwNzMrUMdUFwAwa9asmDdv3lSXYWb2grJ+/fonIqKz2brnRbjPmzeP3t7eqS7DzOwFRdJjw63ztIyZWYEc7mZmBXK4m5kVyOFuZlYgh7uZWYEc7mZmBXK4m5kVyOFuZlagWuEu6Y8kPSjpAUk3SJom6RhJ90jaKumLkg7MsQflcl+un9fOG2BmZvsb9RuqkmYD7weOi4ifSFoBnAOcBVweETdK+jtgKXBVnj8ZEcdKOgf4FPDOsRZ20p9cN9aLTIr1l5031SWYmY2q7r8f6ABeKulnwMHA48BpwG/n+uXAx6jCfWG2AW4GrpSkeJH95NO2P//lqS6hqaM/snmqSzCzSTBquEfEv0j6DLAN+AnwdWA9sDsinslh/cDsbM8Gtudln5G0BzgceKLFtVsbnfJ/TpnqEpr65vu+OdUlmL0g1JmWOYzqaPwYYDdwE3Bmk6GDR+YaYV3jdnuAHoCjjz66Zrlmo7vrTW+e6hKG9ea775rqEuxFos60zOnAIxExACDpy8AbgBmSOvLofQ6wI8f3A3OBfkkdwHTgh0M3GhHLgGUA3d3dL6opG7ORXPmBr051CU1d9L/eVmvcx999dpsrGZ8Pfe7mqS5hUtX5tMw2YL6kgyUJWAA8BNwJDN6LS4CV2V6Vy+T6O15s8+1mZlNt1HCPiHuo3hjdAGzOyywDPghcIqmPak796rzI1cDh2X8JcGkb6jYzsxHU+rRMRHwU+OiQ7oeBk5uMfRpYPPHSzMxsvJ4Xv8RkZvZ8seXjd0x1CU295kOnjWm8//2AmVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWoFHDXdKrJG1sOP1I0sWSZkpaLWlrnh+W4yXpCkl9kjZJOrH9N8PMzBrV+Q3V70XECRFxAnASsBe4heq3UddERBewhud+K/VMoCtPPcBV7SjczMyGN9ZpmQXA9yPiMWAhsDz7lwOLsr0QuC4q64AZko5qSbVmZlbLWMP9HOCGbB8ZEY8D5PkR2T8b2N5wmf7s24ekHkm9knoHBgbGWIaZmY2kdrhLOhB4O3DTaEOb9MV+HRHLIqI7Iro7OzvrlmFmZjWM5cj9TGBDROzM5Z2D0y15viv7+4G5DZebA+yYaKFmZlbfWML9XJ6bkgFYBSzJ9hJgZUP/efmpmfnAnsHpGzMzmxwddQZJOhj4deD3Gro/CayQtBTYBizO/tuAs4A+qk/WXNCyas3MrJZa4R4Re4HDh/T9gOrTM0PHBnBhS6ozM7Nx8TdUzcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAtUKd0kzJN0s6buStkh6vaSZklZL2prnh+VYSbpCUp+kTZJObO9NMDOzoeoeuf9v4PaIeDVwPLAFuBRYExFdwJpchuqHtLvy1ANc1dKKzcxsVKOGu6SXA28CrgaIiJ9GxG5gIbA8hy0HFmV7IXBdVNYBMyQd1fLKzcxsWHWO3H8RGAD+n6T7JH1W0iHAkRHxOECeH5HjZwPbGy7fn31mZjZJ6oR7B3AicFVEvBb4N56bgmlGTfpiv0FSj6ReSb0DAwO1ijUzs3rqhHs/0B8R9+TyzVRhv3NwuiXPdzWMn9tw+TnAjqEbjYhlEdEdEd2dnZ3jrd/MzJoYNdwj4l+B7ZJelV0LgIeAVcCS7FsCrMz2KuC8/NTMfGDP4PSNmZlNjo6a494HfF7SgcDDwAVUfxhWSFoKbAMW59jbgLOAPmBvjjUzs0lUK9wjYiPQ3WTVgiZjA7hwgnWZmdkE+BuqZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFqhXukh6VtFnSRkm92TdT0mpJW/P8sOyXpCsk9UnaJOnEdt4AMzPb31iO3H8tIk6IiMGf27sUWBMRXcCaXAY4E+jKUw9wVauKNTOzeiYyLbMQWJ7t5cCihv7rorIOmCHpqAlcj5mZjVHdcA/g65LWS+rJviMj4nGAPD8i+2cD2xsu2599+5DUI6lXUu/AwMD4qjczs6Y6ao47JSJ2SDoCWC3puyOMVZO+2K8jYhmwDKC7u3u/9WZmNn61jtwjYkee7wJuAU4Gdg5Ot+T5rhzeD8xtuPgcYEerCjYzs9GNGu6SDpF06GAbOAN4AFgFLMlhS4CV2V4FnJefmpkP7BmcvjEzs8lRZ1rmSOAWSYPjvxARt0u6F1ghaSmwDVic428DzgL6gL3ABS2v2szMRjRquEfEw8DxTfp/ACxo0h/AhS2pzszMxsXfUDUzK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK1DtcJd0gKT7JN2ay8dIukfSVklflHRg9h+Uy325fl57Sjczs+GM5cj9D4EtDcufAi6PiC7gSWBp9i8FnoyIY4HLc5yZmU2iWuEuaQ7wVuCzuSzgNODmHLIcWJTthblMrl+Q483MbJLUPXL/G+BPgWdz+XBgd0Q8k8v9wOxszwa2A+T6PTl+H5J6JPVK6h0YGBhn+WZm1syo4S7pN4BdEbG+sbvJ0Kix7rmOiGUR0R0R3Z2dnbWKNTOzejpqjDkFeLuks4BpwMupjuRnSOrIo/M5wI4c3w/MBfoldQDTgR+2vHIzMxvWqEfuEfHfI2JORMwDzgHuiIh3AXcCZ+ewJcDKbK/KZXL9HRGx35G7mZm1z0Q+5/5B4BJJfVRz6ldn/9XA4dl/CXDpxEo0M7OxqjMt858iYi2wNtsPAyc3GfM0sLgFtZmZ2Tj5G6pmZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmB6vxA9jRJ35F0v6QHJf1Z9h8j6R5JWyV9UdKB2X9QLvfl+nntvQlmZjZUnSP3fwdOi4jjgROAt0iaD3wKuDwiuoAngaU5finwZEQcC1ye48zMbBLV+YHsiIincvEleQrgNODm7F8OLMr2wlwm1y+QpJZVbGZmo6o15y7pAEkbgV3AauD7wO6IeCaH9AOzsz0b2A6Q6/dQ/YD20G32SOqV1DswMDCxW2FmZvuoFe4R8fOIOAGYQ/Wj2K9pNizPmx2lx34dEcsiojsiujs7O+vWa2ZmNYzp0zIRsRtYC8wHZkjqyFVzgB3Z7gfmAuT66cAPW1GsmZnVU+fTMp2SZmT7pcDpwBbgTuDsHLYEWJntVblMrr8jIvY7cjczs/bpGH0IRwHLJR1A9cdgRUTcKukh4EZJfwncB1yd468GrpfUR3XEfk4b6jYzsxGMGu4RsQl4bZP+h6nm34f2Pw0sbkl1ZmY2Lv6GqplZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZger8hupcSXdK2iLpQUl/mP0zJa2WtDXPD8t+SbpCUp+kTZJObPeNMDOzfdU5cn8G+EBEvAaYD1wo6TjgUmBNRHQBa3IZ4EygK089wFUtr9rMzEY0arhHxOMRsSHbPwa2ALOBhcDyHLYcWJTthcB1UVkHzJB0VMsrNzOzYY1pzl3SPKofy74HODIiHofqDwBwRA6bDWxvuFh/9g3dVo+kXkm9AwMDY6/czMyGVTvcJb0M+BJwcUT8aKShTfpiv46IZRHRHRHdnZ2ddcswM7MaaoW7pJdQBfvnI+LL2b1zcLolz3dlfz8wt+Hic4AdrSnXzMzqqPNpGQFXA1si4q8bVq0ClmR7CbCyof+8/NTMfGDP4PSNmZlNjo4aY04B3gNslrQx+/4H8ElghaSlwDZgca67DTgL6AP2Ahe0tGIzMxvVqOEeEf9E83l0gAVNxgdw4QTrMjOzCfA3VM3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswLV+Zm9ayTtkvRAQ99MSaslbc3zw7Jfkq6Q1Cdpk6QT21m8mZk1V+fI/VrgLUP6LgXWREQXsCaXAc4EuvLUA1zVmjLNzGwsRg33iLgb+OGQ7oXA8mwvBxY19F8XlXXADElHtapYMzOrZ7xz7kdGxOMAeX5E9s8GtjeM68++/UjqkdQrqXdgYGCcZZiZWTOtfkO12Q9pR7OBEbEsIrojoruzs7PFZZiZvbiNN9x3Dk635Pmu7O8H5jaMmwPsGH95ZmY2HuMN91XAkmwvAVY29J+Xn5qZD+wZnL4xM7PJ0zHaAEk3AKcCsyT1Ax8FPgmskLQU2AYszuG3AWcBfcBe4II21GxmZqMYNdwj4txhVi1oMjaACydalJmZTYy/oWpmViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mViCHu5lZgRzuZmYFcribmRXI4W5mVqC2hLukt0j6nqQ+SZe24zrMzGx4LQ93SQcA/xc4EzgOOFfSca2+HjMzG147jtxPBvoi4uGI+ClwI7CwDddjZmbDUPWb1i3coHQ28JaI+G+5/B7gdRFx0ZBxPUBPLr4K+F5LC9nXLOCJNm6/3Vz/1Hkh1w6uf6q1u/5XRERnsxUdbbgyNenb7y9IRCwDlrXh+vcjqTciuifjutrB9U+dF3Lt4Pqn2lTW345pmX5gbsPyHGBHG67HzMyG0Y5wvxfoknSMpAOBc4BVbbgeMzMbRsunZSLiGUkXAV8DDgCuiYgHW309YzQp0z9t5Pqnzgu5dnD9U23K6m/5G6pmZjb1/A1VM7MCOdzNzApUVLhL+tZU1zARkuZJemCq63gxkfR+SVskfX6qaxmNpI9J+mNJfy7p9Em4vkXt+Ha5pEclzWr1dp+vJHVLuiLb50u6Mttt2b+D2vE59ykTEW+Y6hqsNSSJ6j2hZ9t8VX8AnBkRj4x3A5IOiIift7CmEUXERybpqhYBtwIPTdL1FSkieoHeJqvaun9LO3J/SpXLJD0gabOkd+a66yUtbBj7eUlvb1Mdh0j6R0n3Zx3vlPQRSffm8rIMLySdlOO+DVzYsI3zJX1Z0u2Stkr6dMO6MyR9W9IGSTdJeln2f1LSQ5I2SfpM9i3O67xf0t0tuG1fkbRe0oP5LePB/f7xvI51ko7M/lfm8r15tPlUw3b+JPs3Sfqz7JuXR9F/C2xg3+9LtJykvwN+EVgl6UOSrsma7ht8rGRN38h9vUHSG7L/VEl3SvoCsLmNNX4o/wnf/6f6JjeSrs1vgg93nzfd71nzrQ3bvlLS+c22k7fz7cBlkjZKeuU469/vuZCr3pf7c7OkV+fYkyV9K/f/tyQN3t7z83H3VUmPSLpI0iU5bp2kmQ23+/Z8fH5jcLvtIOl/SvqupNWSblD1imqtpO5cP0vSo9neZ79nX0v274giopgT8BTwDmA11ccwjwS2AUcBbwa+kuOmA48AHW2q4x3APzQsTwdmNixfD7wt25uAN2f7MuCBbJ8PPJyXnQY8RhV2s4C7gUNy3AeBjwAzqf6Fw+AnoGbk+WZgdmPfBG/bzDx/KfAAcDjVN5AHb8+ngQ9n+1bg3Gy/F3gq22dQfURMVAcYtwJvAuYBzwLzJ/Ex82ju078C3j24n4B/Bg4BDgamZX8X0JvtU4F/A45pY20n5f13MPByoA/4Y+Ba4OwR7vPh9vupwK0N278yH2fDbeda4Ow2PBceBd6Xy38AfDbbLyefk8DpwJcangt9wKFAJ7AHeG+uuxy4ONtrgK5svw64o033SzewMZ8DhwJb835ZC3TnmFnAo0P3e96WK1u1f0c6FXXknt4I3BARP4+IncBdwK9ExF3AsZKOAM6leuA806YaNgOnS/qUpF+NiD3Ar0m6R9Jm4DTglyRNp3oi3ZWXu37IdtZExJ6IeJrqpdsrgPlU/23zm5I2Akuy/0fA08BnJf0WsDe38U3gWkm/S/UHb6LeL+l+YB3VH5su4KdUgQKwniqkAV4P3JTtLzRs44w83Ud1hP7q3A7AYxGxrgV1jtUZwKW5T9dS/UE9GngJ8A95v91Ete8HfScmMJ1Tw68Ct0TE3oj4Eft/GXC4+3y4/T6c4bbTCs2eCwBfzvPGx8t04CZV7ztdDvxSw3bujIgfR8QAVbh/tWH78/LV6xvy8huBv6c6qGuHNwIrI+InEfHjhlqeV4qac0/N/rfNoOuBd1F9a/Z32lVARPyzpJOAs4BPSPo61ZRLd0Rsl/QxqvAQTf7vToN/b2j/nOr+ErA6Is4dOljSycACqtt3EXBaRLxX0uuAtwIbJZ0QET8Yz+2SdCrVEdXrI2KvpLV5O34WeSjSUOeImwI+ERF/P2T786iOhqeCgHdExD7/wC7vq53A8VSvMp5uWD0ZtQ77+IjqC4P73ecjbOsZ9p2KnTbO7dQ2zHMBnntsNz5e/oIqxH8zHwtrGzbV+Fx4tmH52bz8LwC7I+KEVtQ9iuEypnH/TpuEOkZU4pH73cA7JR0gqZPq5f53ct21wMUA0cZvzUr6L8DeiPgc8BngxFz1RB5hnJ017Ab2SHpjrn9Xjc2vA06RdGxe18GS/mtud3pE3EZ1G0/I9a+MiHuiehPuCSY2jz0deDKD/dVUryJGq/Ud2T6nof9rwO/oufcKZucrqqn0Nap54MH3Ql6b/dOBx6N6Y/c9tObVT113A78p6aWSDgXe1rhyuPuc4ff7Y8Bxkg7KV40LRtnOj6mmHcZthOdCM9OBf8n2+WO5nnxl84ikxXm9knT82Cuu5Z+At0malvvurdn/KNVUGuRzfBQT3r8jKe3IPYBbqF6W3p/LfxoR/woQETslbQG+0uY6fpnqjZJngZ8Bv0/1zvhmqgfAvQ1jLwCukbSXKmBGFBED+SbYDZIOyu4PUz1QVkoafEXwR7nuMkld2beGar+M1+3AeyVtopqjHW365GLgc5I+APwj1ctpIuLrkl4DfDuz9Cng3VRHcVPlL4C/ATZlwD8K/Abwt8CXMjTuZBJfWUTEBklfpJrffQz4xpAhh9L8Ph9uv2+XtILqfZ6tVNNiI23nRqopqfdTzQ1/fxw3o9lz4eZhxn4aWC7pEuCOcVzXu4CrJH2YajrtRib2eG8qIu6VtCq3/RjVJ2H2UP3xWqHq35zXqb8V+3dYxfz7AUmHAxsi4hUjjDmYKmBPbJj7szbJ/f2TiAhJ51C9yecfbmkz7/f2k/SyiHgq9/XdQE9EbJjquhoVceSeL/3WUv3lHG7M6cA1wF872CfNScCVeSS8mza+z2H78H5vv2WqvoA0DVj+fAt2KOjI3czMnlPiG6pmZi96DnczswI53M3MCuRwNzMrkMPdzKxA/wHH7CfcqeGNcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dd = pd.Series(y_train).value_counts()\n",
    "\n",
    "# dd\n",
    "sns.barplot(x=np.array(['joy','sadness','anger','fear','disgust','shame','guilt']),y=dd.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tockenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tockenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
    "            \n",
    "            \n",
    "    print(type(y_train))\n",
    "            \n",
    "#     encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
    "#     encoded_test = [1 if label =='positive' else 0 for label in y_val] \n",
    "    \n",
    "    \n",
    "    \n",
    "#     #NAME_CONTRACT_TYPE ->catagorical to numerical\n",
    "\n",
    "    emotion = {        \n",
    "        'joy':1,\n",
    "        'sadness':2,\n",
    "        'anger':3,\n",
    "        'fear': 4,\n",
    "        'shame':5,\n",
    "        'disgust':6,\n",
    "        'guilt':0\n",
    "    }\n",
    "    encoded_train = [emotion[label] for label in y_train]\n",
    "\n",
    "\n",
    "    encoded_test = [emotion[label] for label in y_val]\n",
    "\n",
    "\n",
    "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 6, ..., 2, 1, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, ..., 0, 5, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary is 1000\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of vocabulary is {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    5584.000000\n",
       "mean        7.243016\n",
       "std         4.785779\n",
       "min         0.000000\n",
       "25%         4.000000\n",
       "50%         6.000000\n",
       "75%        10.000000\n",
       "max        52.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_len = [len(i) for i in x_train]\n",
    "pd.Series(rev_len).hist()\n",
    "plt.show()\n",
    "pd.Series(rev_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_pad = padding_(x_train, 9)\n",
    "x_test_pad = padding_(x_test,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 150\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([376,  32,  82, 211,   2, 995, 110, 151, 108], dtype=torch.int32),\n",
       " tensor(1, dtype=torch.int32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5584,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    811\n",
       "2    806\n",
       "3    802\n",
       "4    797\n",
       "6    794\n",
       "5    794\n",
       "0    780\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([150, 9])\n",
      "Sample input: \n",
      " tensor([[754, 125, 235,  ...,  10, 637,  76],\n",
      "        [  0,   0,   0,  ...,   0, 283,  18],\n",
      "        [  1,  26, 122,  ..., 368, 606, 522],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ...,   0, 111, 791],\n",
      "        [  0,   0,   0,  ..., 276,   1,  29],\n",
      "        [  0,   0,   0,  ...,   0, 133,   3]], dtype=torch.int32)\n",
      "Sample input: \n",
      " tensor([2, 5, 4, 2, 4, 0, 6, 2, 5, 5, 2, 0, 5, 5, 1, 5, 5, 5, 0, 6, 3, 4, 3, 2,\n",
      "        6, 2, 4, 4, 2, 0, 1, 4, 6, 0, 1, 0, 1, 4, 4, 1, 6, 5, 5, 5, 5, 5, 6, 1,\n",
      "        5, 2, 6, 6, 1, 1, 1, 3, 4, 6, 6, 4, 3, 4, 4, 0, 0, 3, 5, 3, 3, 6, 3, 5,\n",
      "        0, 0, 4, 1, 1, 2, 2, 1, 3, 1, 2, 4, 1, 5, 5, 3, 0, 0, 5, 2, 3, 2, 3, 3,\n",
      "        0, 5, 4, 1, 5, 6, 1, 3, 0, 2, 6, 6, 5, 0, 1, 4, 6, 0, 4, 3, 4, 1, 3, 2,\n",
      "        4, 6, 2, 3, 1, 3, 1, 6, 6, 1, 0, 6, 6, 6, 3, 5, 4, 0, 5, 3, 6, 1, 4, 1,\n",
      "        4, 0, 6, 4, 2, 2], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size())\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentLSTM,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        \n",
    "        h = model.init_hidden(batch_size)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(1001, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=7, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 7\n",
    "hidden_dim = 256\n",
    "\n",
    "model = SentimentLSTM(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-14329c2323b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m-> 1121\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2822\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2824\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "num_epochs  = 5\n",
    "\n",
    "loss_list = []\n",
    "iteration_list_train = []\n",
    "count_train = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    h = model.init_hidden(batch_size)\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            outputs , h = model(inputs , h)\n",
    "            \n",
    "            print(epoch , i )\n",
    "\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            \n",
    "\n",
    "            loss_list.append(loss.data) \n",
    "\n",
    "            # Backward and optimize\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
