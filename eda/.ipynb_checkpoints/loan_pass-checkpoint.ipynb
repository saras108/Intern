{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "csv_dir = '../../loan_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File loan_data\\application_train.csv does not exist: 'loan_data\\\\application_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d08a74b2d167>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'application_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'application_test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Train data shape : {data_train.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Test data shape : {data_test.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File loan_data\\application_train.csv does not exist: 'loan_data\\\\application_train.csv'"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(os.path.join(csv_dir,'application_train.csv'))\n",
    "data_test = pd.read_csv(os.path.join(csv_dir,'application_test.csv'))\n",
    "\n",
    "print(f'Train data shape : {data_train.shape}')\n",
    "print(f'Test data shape : {data_test.shape}')\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['TARGET'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_table(df):\n",
    "\n",
    "    #get total missing values\n",
    "    missing_value = df.isnull().sum()\n",
    "    \n",
    "    #get the percentage of missing values\n",
    "    missing_values_percentage = 100*missing_value/len(df)\n",
    "    \n",
    "    #Make a table with result\n",
    "    miss_value_table = pd.concat([missing_value , missing_values_percentage] , axis = 1)\n",
    "    \n",
    "    #Rename table column\n",
    "    miss_value_table.rename(columns = {\n",
    "        0: \"Missing Counts\",\n",
    "        1: \"Missing Percentage\"\n",
    "    } , inplace = True)\n",
    "    \n",
    "    #Sort the table\n",
    "    miss_value_table = miss_value_table[miss_value_table.iloc[: , 1]!=0].sort_values('Missing Percentage' , ascending = False )\n",
    "    \n",
    "    #Print the information\n",
    "    print(\"The selected dataframe has \" + str(df.shape[1]) + \"Columns. \\n\"\n",
    "         \"There are \" +str(miss_value_table.shape[0]) + \" columns have a missing values.\")\n",
    "    \n",
    "    return miss_value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = missing_value_table(data_train)\n",
    "# missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_selected_column = ['COMMONAREA_MEDI',\n",
    "                        'COMMONAREA_MODE',\n",
    "                        'NONLIVINGAPARTMENTS_MEDI',\n",
    "                        'NONLIVINGAPARTMENTS_MODE',\n",
    "                        'FONDKAPREMONT_MODE',\n",
    "                        'LIVINGAPARTMENTS_MODE',\n",
    "                        'LIVINGAPARTMENTS_MEDI',\n",
    "                        'FLOORSMIN_MODE',\n",
    "                        'FLOORSMIN_MEDI',\n",
    "                        'YEARS_BUILD_MODE' ,\n",
    "                         'YEARS_BUILD_MEDI',\n",
    "                        'LANDAREA_MEDI',\n",
    "                         'LANDAREA_MODE',\n",
    "                        'BASEMENTAREA_MEDI',\n",
    "                        'BASEMENTAREA_MODE',\n",
    "                        'NONLIVINGAREA_MEDI',\n",
    "                        'NONLIVINGAREA_MODE',\n",
    "                        'ELEVATORS_MEDI',\n",
    "                        'ELEVATORS_MODE',\n",
    "                        'WALLSMATERIAL_MODE',\n",
    "                        'APARTMENTS_MODE',\n",
    "                        'APARTMENTS_MEDI',\n",
    "                        'ENTRANCES_MODE',\n",
    "                        'ENTRANCES_MEDI',\n",
    "                        'LIVINGAREA_MEDI',\n",
    "                        'LIVINGAREA_MODE',\n",
    "                        'FLOORSMAX_MEDI',\n",
    "                        'FLOORSMAX_MODE',\n",
    "                        'YEARS_BEGINEXPLUATATION_MEDI',\n",
    "                        'YEARS_BEGINEXPLUATATION_MODE',\n",
    "                        'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "                        'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "                        'AMT_REQ_CREDIT_BUREAU_MON',\n",
    "                        'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    "                        'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "                        'OBS_30_CNT_SOCIAL_CIRCLE',\n",
    "                        'DEF_30_CNT_SOCIAL_CIRCLE',\n",
    "                        'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "                        'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    "                        'SK_ID_CURR',\n",
    "                        'AMT_REQ_CREDIT_BUREAU_YEAR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_colm(df):\n",
    "    df.drop(drop_selected_column , axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_colm(data_train)\n",
    "drop_colm(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all the data of the column starting with “FLAG_DOCUMENT” \n",
    "\n",
    "def get_docs_count(df):\n",
    "    df['DOCS_COUNT'] = df.filter(regex = '^(FLAG_DOCUMENT)').sum(axis = 1)\n",
    "    \n",
    "    drop_these = df.filter(regex = '^(FLAG_DOCUMENT)').columns\n",
    "    \n",
    "    df.drop(drop_these , axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_docs_count(data_train)\n",
    "get_docs_count(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = missing_value_table(data_train)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_zero(df , col):\n",
    "    for c in col:\n",
    "        df[c] = df[c].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_zero(data_train , ['COMMONAREA_AVG', 'NONLIVINGAPARTMENTS_AVG','LIVINGAPARTMENTS_AVG', 'FLOORSMIN_AVG' , 'YEARS_BUILD_AVG' , 'LANDAREA_AVG','BASEMENTAREA_AVG','NONLIVINGAREA_AVG','ELEVATORS_AVG','APARTMENTS_AVG','ENTRANCES_AVG','LIVINGAREA_AVG','HOUSETYPE_MODE','FLOORSMAX_AVG','YEARS_BEGINEXPLUATATION_AVG','TOTALAREA_MODE','EMERGENCYSTATE_MODE','OWN_CAR_AGE'])\n",
    "impute_zero(data_test , ['COMMONAREA_AVG', 'NONLIVINGAPARTMENTS_AVG','LIVINGAPARTMENTS_AVG', 'FLOORSMIN_AVG' , 'YEARS_BUILD_AVG' , 'LANDAREA_AVG','BASEMENTAREA_AVG','NONLIVINGAREA_AVG','ELEVATORS_AVG','APARTMENTS_AVG','ENTRANCES_AVG','LIVINGAREA_AVG','HOUSETYPE_MODE','FLOORSMAX_AVG','YEARS_BEGINEXPLUATATION_AVG','TOTALAREA_MODE','EMERGENCYSTATE_MODE' , 'OWN_CAR_AGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = missing_value_table(data_train)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique Entries of the object(Catagorical) Column\n",
    "def get_unique_catagory(df):\n",
    "    return df.select_dtypes('object').apply(pd.Series.nunique , axis=0)\n",
    "\n",
    "get_unique_catagory(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAME_CONTRACT_TYPE ->catagorical to numerical\n",
    "\n",
    "contract_type = {\n",
    "    'Cash loans':0,\n",
    "    'Revolving loans':1\n",
    "}\n",
    "data_train.NAME_CONTRACT_TYPE = [contract_type[item] for item in data_train.NAME_CONTRACT_TYPE]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = data_train[\"CODE_GENDER\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = {\n",
    "    'M' : 0,\n",
    "    'F' : 1,\n",
    "    'XNA' : 1\n",
    "}\n",
    "\n",
    "data_train.CODE_GENDER = [gender[item] for item in data_train.CODE_GENDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = {\n",
    "    'N' : 0,\n",
    "    'Y' : 1\n",
    "}\n",
    "\n",
    "data_train.FLAG_OWN_REALTY = [flags[item] for item in data_train.FLAG_OWN_REALTY]\n",
    "\n",
    "\n",
    "data_train.FLAG_OWN_CAR = [flags[item] for item in data_train.FLAG_OWN_CAR]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = data_train[\"EMERGENCYSTATE_MODE\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newflags = {\n",
    "    'No' : 0,\n",
    "    'Yes' : 1,\n",
    "    0 :0\n",
    "}\n",
    "\n",
    "data_train.EMERGENCYSTATE_MODE = [newflags[item] for item in data_train.EMERGENCYSTATE_MODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = pd.DataFrame()\n",
    "car['flag'] = data_train['FLAG_OWN_CAR']\n",
    "car['age'] = data_train['OWN_CAR_AGE']\n",
    "\n",
    "car[(car['flag'] == 0) &  (car['age']  > 0)] #shouls be empty\n",
    "\n",
    "a = car[(car['age']==0)  & (car['flag'] == 1)] #Should be empty else we will update age as 0.5\n",
    "\n",
    "select_row = a.index\n",
    "\n",
    "#Updating  the selected rows in the dataframe\n",
    "data_train.at[select_row,'OWN_CAR_AGE']=0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Deleting the 'FLAG_OWN_CAR' column as the data on is represented on 'OWN_CAR_AGE'\n",
    "data_train.drop(['OWN_CAR_AGE'] , axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Delete the car dataframe\n",
    "del car\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = data_train[\"CNT_CHILDREN\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child = pd.DataFrame()\n",
    "\n",
    "child['count'] = data_train[(data_train['CNT_CHILDREN'] > 7)]['CNT_CHILDREN']\n",
    "child['lives_with'] = data_train.loc[child.index,'NAME_TYPE_SUITE'] \n",
    "\n",
    "child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train['NAME_TYPE_SUITE'] = data_train['NAME_TYPE_SUITE'].fillna(0)\n",
    "\n",
    "a = data_train.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1)\n",
    "a ['NAME_FAMILY_STATUS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data['children'] = data_train['CNT_CHILDREN']\n",
    "data['suite'] = data_train['NAME_TYPE_SUITE']\n",
    "data['housing'] = data_train['NAME_HOUSING_TYPE']\n",
    "data['status'] = data_train['NAME_FAMILY_STATUS']\n",
    "data['income'] = data_train['NAME_INCOME_TYPE']\n",
    "data['education'] = data_train['NAME_EDUCATION_TYPE']\n",
    "data['occupation'] = data_train['OCCUPATION_TYPE']\n",
    "data['organization'] = data_train['ORGANIZATION_TYPE']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(a , cmap = plt.cm.RdYlBu_r , annot=True)\n",
    "plt.title(\"Corelation heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictn = {\n",
    "    'Spouse, partner' : 'Family',\n",
    "    'Children' : 'Family',\n",
    "    'Family' : 'Family',\n",
    "    'Unaccompanied' : 'Unaccompanied',\n",
    "    'Other_A' : 'Group',\n",
    "    'Other_B' : 'Group',\n",
    "    'Group of people' : 'Group',\n",
    "    np.nan : 'Amissing'\n",
    "}\n",
    "\n",
    "data_train.NAME_TYPE_SUITE = [dictn[item] for item in data_train.NAME_TYPE_SUITE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = data_train[\"NAME_TYPE_SUITE\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "data_train = pd.get_dummies(data_train, columns=['NAME_TYPE_SUITE'], drop_first=True,  prefix='SUITE',)\n",
    "\n",
    "# Drop NAME_TYPE_SUITE column\n",
    "# data_train.drop(['NAME_TYPE_SUITE'] , axis = 1, inplace = True) #No need to drop as one hot encoding does for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_catagory(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_table(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME_INCOME_TYPE               \n",
    "\n",
    "item_counts = data_train[\"NAME_INCOME_TYPE\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"NAME_INCOME_TYPE\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newda = pd.DataFrame()\n",
    "newda['pension'] = data_train[(data_train['NAME_INCOME_TYPE'] == 'Unemployed')]['NAME_INCOME_TYPE']\n",
    "newda['target'] =  data_train.loc[newda.index,'TARGET'] \n",
    "\n",
    "print(newda[\"target\"].value_counts())\n",
    "\n",
    "newda['target'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newflags = {\n",
    "    'Working' : 2 ,\n",
    "    'Commercial associate' : 3,\n",
    "    'Businessman' : 3,\n",
    "    'Pensioner' : 1,\n",
    "    'State servant' : 4,\n",
    "    'Unemployed' : 0,\n",
    "    'Student' : 0,\n",
    "    'Maternity leave' : 1,\n",
    "}\n",
    "\n",
    "data_train.NAME_INCOME_TYPE = [newflags[item] for item in data_train.NAME_INCOME_TYPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME_INCOME_TYPE               \n",
    "\n",
    "item_counts = data_train[\"NAME_INCOME_TYPE\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME_EDUCATION_TYPE    \n",
    " \n",
    "item_counts = data_train[\"NAME_EDUCATION_TYPE\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newflags = {\n",
    "    'Secondary / secondary special' : 0 ,\n",
    "    'Higher education' : 1,\n",
    "    'Incomplete higher' : 2,\n",
    "    'Lower secondary' : 3,\n",
    "    'Academic degree' : 4\n",
    "}\n",
    "\n",
    "data_train.NAME_EDUCATION_TYPE = [newflags[item] for item in data_train.NAME_EDUCATION_TYPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME_EDUCATION_TYPE    \n",
    " \n",
    "item_counts = data_train[\"NAME_EDUCATION_TYPE\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get avg salary for every occupation\n",
    "\n",
    "def get_sal(df , col , item_counts):\n",
    "    avg = []\n",
    "    for c in item_counts.index:\n",
    "        a = df[(df[col] == c)]['AMT_ANNUITY'].mean()\n",
    "        avg.append(a)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCCUPATION_TYPE            \n",
    "\n",
    "item_counts = data_train[\"OCCUPATION_TYPE\"].value_counts()\n",
    "item_counts\n",
    "\n",
    "dtable= pd.DataFrame(data=item_counts)\n",
    "\n",
    "avrg = get_sal(data_train , 'OCCUPATION_TYPE' ,item_counts )\n",
    "    \n",
    "dtable['AVERAGE INCOME'] = avrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtable = dtable.sort_values(by=['AVERAGE INCOME'])\n",
    "dtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtable.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flag(dtable):\n",
    "    flags = {0:0 ,}\n",
    "    for data in dtable.index:\n",
    "        newdata = dtable.loc[data ,'AVERAGE INCOME']\n",
    "        if(newdata < 25000):\n",
    "            flags[data] = (1)\n",
    "        elif((newdata > 25000) & (newdata <27000)):\n",
    "            flags[data] = (2)\n",
    "        elif((newdata > 27000) & (newdata < 30000)):\n",
    "            flags[data] = (3)\n",
    "        else:\n",
    "            flags[data] = (4)\n",
    "    return flags   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_flags = get_flag(dtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill all nan with 0\n",
    "data_train['OCCUPATION_TYPE'] = data_train['OCCUPATION_TYPE'].fillna(0)\n",
    "\n",
    "#ordinal encoding\n",
    "data_train.OCCUPATION_TYPE = [job_flags[item] for item in data_train.OCCUPATION_TYPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corelation  between occupation type, Education and NAME_INCOME_TYPE\n",
    "\n",
    "test_data = data_train[['TARGET' , 'OCCUPATION_TYPE' , 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE']]\n",
    "\n",
    "tst_crr = test_data.corr()\n",
    "\n",
    "tst_crr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(tst_crr , cmap = plt.cm.RdYlBu_r , annot=True)\n",
    "plt.title(\"Corelation heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORGANIZATION_TYPE            \n",
    "\n",
    "item_counts = data_train[\"ORGANIZATION_TYPE\"].value_counts()\n",
    "item_counts\n",
    "\n",
    "dtable= pd.DataFrame(data=item_counts)\n",
    "\n",
    "avrg = get_sal(data_train , 'ORGANIZATION_TYPE' ,item_counts )\n",
    "    \n",
    "dtable['AVERAGE INCOME'] = avrg\n",
    "\n",
    "dtable = dtable.sort_values(by=['AVERAGE INCOME'])\n",
    "dtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"ORGANIZATION_TYPE\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_flags = get_flag(dtable)\n",
    "#ordinal encoding\n",
    "data_train.ORGANIZATION_TYPE = [job_flags[item] for item in data_train.ORGANIZATION_TYPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corelation  between occupation type, Education and NAME_INCOME_TYPE\n",
    "\n",
    "test_data = data_train[['TARGET' , 'ORGANIZATION_TYPE' ,'OCCUPATION_TYPE' , 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE']]\n",
    "\n",
    "tst_crr = test_data.corr()\n",
    "\n",
    "tst_crr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(tst_crr , cmap = plt.cm.RdYlBu_r , annot=True)\n",
    "plt.title(\"Corelation heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = data_train[\"HOUSETYPE_MODE\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(df , col):\n",
    "    one_count = []\n",
    "    zero_count = []\n",
    "    \n",
    "    for c in (df.index):\n",
    "        a = data_train[(data_train[col] == c)]['TARGET'].value_counts()\n",
    "        \n",
    "#         if loop vitra if elif lagauda  kamm gareko xaina\n",
    "        \n",
    "        if(len(a.index) == 2):\n",
    "            if(a.index[0] == 0):\n",
    "                zero_count.append(a[0])\n",
    "            if(a.index[0] == 1):\n",
    "                one_count.append(a[0])\n",
    "            if(a.index[1] == 0):\n",
    "                zero_count.append(a[1])\n",
    "            if(a.index[1] == 1):\n",
    "                one_count.append(a[1])\n",
    "        \n",
    "        else:\n",
    "            if(a.index[0] == 0):\n",
    "                zero_count.append(a[0])\n",
    "            else:\n",
    "                zero_count.append(0)\n",
    "            if(a.index[0] == 1):\n",
    "                one_count.append(a[0])\n",
    "            else:\n",
    "                one_count.append(0)\n",
    "            \n",
    "#     print(one_count , zero_count)\n",
    "            \n",
    "        \n",
    "#         one_count.append(a[1])\n",
    "#         zero_count.append(a[0])\n",
    "    \n",
    "    df['one_count'] = one_count\n",
    "    df['zero_count'] = zero_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOUSETYPE_MODE            \n",
    "\n",
    "item_counts = data_train[\"HOUSETYPE_MODE\"].value_counts()\n",
    "item_counts\n",
    "\n",
    "dtable= pd.DataFrame(data=item_counts)\n",
    "\n",
    "avrg = get_sal(data_train , 'HOUSETYPE_MODE' ,item_counts )\n",
    "    \n",
    "dtable['AVERAGE INCOME'] = avrg\n",
    "\n",
    "get_count(dtable , 'HOUSETYPE_MODE')\n",
    "\n",
    "dtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtable.plot.bar(rot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put 0 for 0 and block of flats and put specific housing and trraced house under 1\n",
    "\n",
    "newflags = {\n",
    "    'block of flats' : 0 ,\n",
    "    'specific housing' : 1,\n",
    "    'terraced house' : 1,\n",
    "    0: 0\n",
    "}\n",
    "\n",
    "data_train.HOUSETYPE_MODE = [newflags[item] for item in data_train.HOUSETYPE_MODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME_FAMILY_STATUS             \n",
    "\n",
    "item_counts = data_train[\"NAME_FAMILY_STATUS\"].value_counts()\n",
    "item_counts\n",
    "\n",
    "dtable= pd.DataFrame(data=item_counts)\n",
    "\n",
    "get_count(dtable , 'NAME_FAMILY_STATUS')\n",
    "\n",
    "dtable['paid_precentage'] = 100*dtable['one_count']/dtable['NAME_FAMILY_STATUS']\n",
    "\n",
    "dtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtable.plot.barh(rot=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies((dtable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['NAME_FAMILY_STATUS'] =data_train['NAME_FAMILY_STATUS'].replace('Unknown', 'Widow')\n",
    "\n",
    "item_counts = data_train[\"NAME_FAMILY_STATUS\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.get_dummies(data_train, columns=['NAME_FAMILY_STATUS'], prefix='FAMILY',)\n",
    "\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEEKDAY_APPR_PROCESS_START         \n",
    "\n",
    "item_counts = data_train[\"WEEKDAY_APPR_PROCESS_START\"].value_counts()\n",
    "item_counts\n",
    "\n",
    "dtable= pd.DataFrame(data=item_counts)\n",
    "\n",
    "\n",
    "dtable.plot.barh(rot=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will drop the column as it doesnt seem to have a useful info to get the  targed values\n",
    "\n",
    "data_train.drop(['WEEKDAY_APPR_PROCESS_START'] , axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME_HOUSING_TYPE     \n",
    "\n",
    "item_counts = data_train[\"NAME_HOUSING_TYPE\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtable= pd.DataFrame(data=item_counts)\n",
    "\n",
    "get_count(dtable , 'NAME_HOUSING_TYPE')\n",
    "\n",
    "dtable['paid_precentage'] = 100*dtable['one_count']/dtable['NAME_HOUSING_TYPE']\n",
    "\n",
    "dtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtable.plot.barh(rot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.get_dummies(data_train, columns=['NAME_HOUSING_TYPE'], prefix='HOUSING',)\n",
    "\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =missing_value_table(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3 , nrows=7, figsize=(20, 35))\n",
    "\n",
    "colm = list(a.index)\n",
    "\n",
    "k = 0\n",
    "for i in range(0,7):\n",
    "  for j in range(0,1):\n",
    "    sns.histplot(data = data_train, hue='TARGET' ,x=colm[k], ax=axs[i][j])\n",
    "    sns.histplot(data = data_train,x=colm[k], ax=axs[i][j +2])\n",
    "    sns.boxplot(data = data_train,x=colm[k], ax=axs[i][j+1])\n",
    "\n",
    "    k = k +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "colm = list(a.index)\n",
    "x = data_train[colm].values  #returns a numpy array\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n",
    "\n",
    "\n",
    "colm = list(df.index)\n",
    "fig, axs = plt.subplots(ncols=2 , nrows=7, figsize=(20, 35))\n",
    "\n",
    "k = 0\n",
    "for i in range(0,7):\n",
    "  for j in range(0,1):\n",
    "    sns.boxplot(data = df,x=colm[k], ax=axs[i][j+1])\n",
    "    sns.histplot(data = df ,x=colm[k], ax=axs[i][j])\n",
    "    k = k +1\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_mean(df , col):\n",
    "    mean = df[col].mean()\n",
    "    df[col] = df[col].fillna(mean)\n",
    "#     print(mean)\n",
    "    \n",
    "def impute_median(df , col):\n",
    "    med = df[col].median()\n",
    "    df[col] = df[col].fillna(med)\n",
    "#     print(med)\n",
    "\n",
    "def impute_mode(df, col):\n",
    "    mode = df[col].mode()\n",
    "    df[col] = df[col].fillna(mode[0])\n",
    "#     print(mode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['AMT_GOODS_PRICE'].corr(data_train['AMT_ANNUITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_crr = data_train[['AMT_GOODS_PRICE' , 'AMT_ANNUITY' , 'EXT_SOURCE_1' , 'EXT_SOURCE_2' , 'EXT_SOURCE_3']]\n",
    "new_crr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(new_crr.corr() , cmap = plt.cm.RdYlBu_r , annot=True)\n",
    "plt.title(\"Corelation heatmap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can drop AMT_GOODS_PRICE colm as it is corelated with AMT_ANNUITY\n",
    "\n",
    "data_train.drop(['AMT_GOODS_PRICE'] , axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_table(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_catagory(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_train['DAYS_LAST_PHONE_CHANGE']=abs(data_train['DAYS_LAST_PHONE_CHANGE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_mean(data_train, 'EXT_SOURCE_1')\n",
    "impute_median(data_train, 'EXT_SOURCE_2')\n",
    "impute_median(data_train, 'EXT_SOURCE_3')\n",
    "impute_mean(data_train, 'AMT_ANNUITY')\n",
    "impute_median(data_train, 'CNT_FAM_MEMBERS')\n",
    "impute_mode(data_train, 'DAYS_LAST_PHONE_CHANGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_table(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of age on repayment\n",
    "\n",
    "#find out the corelation of positive days since birth and target\n",
    "\n",
    "data_train['DAYS_BIRTH'] = abs(data_train['DAYS_BIRTH'])\n",
    "data_train['DAYS_BIRTH'].corr(data_train['TARGET'])\n",
    "\n",
    "\n",
    "#as client gets older, there is negative linear regression with target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of Age\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.hist(data_train['DAYS_BIRTH']/365 , edgecolor = 'k' , bins = 25)\n",
    "plt.title('Age of Client')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the effect of age in the target\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(10 , 8))\n",
    "\n",
    "sns.kdeplot(data_train.loc[data_train['TARGET'] == 0 , 'DAYS_BIRTH']/365 , label = \"target = 0\")\n",
    "\n",
    "sns.kdeplot(data_train.loc[data_train['TARGET'] == 1 , 'DAYS_BIRTH']/365 , label = \"target = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age information into seprate dataframe\n",
    "\n",
    "age_data = data_train[['TARGET' , 'DAYS_BIRTH']]\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH']/365\n",
    "\n",
    "#Bin the age data\n",
    "age_data['Year_binded'] = pd.cut(age_data['YEARS_BIRTH'] , bins = np.linspace(20 , 70 , num = 11))\n",
    "age_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_group = age_data.groupby('Year_binded').mean()\n",
    "age_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 12))\n",
    "\n",
    "#Graph the age bins and the avg of the target as a barplot\n",
    "plt.bar(age_group.index.astype(str) , 100*age_group['TARGET'])\n",
    "\n",
    "#Plot labelling \n",
    "plt.xticks(rotation = 75)\n",
    "plt.xlabel('Age group (years)')\n",
    "plt.ylabel('Failure to repay (%)')\n",
    "\n",
    "plt.title('Failure to repay by age group')\n",
    "\n",
    "# There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data_train.corr()['TARGET'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative corelation\n",
    "correlation[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive corelation\n",
    "correlation[36:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
